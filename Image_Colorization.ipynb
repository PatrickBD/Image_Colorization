{
  "cells": [
    {
      "metadata": {
        "_uuid": "786ae4ce8ebfcbdf70e86070cbae167065eda4cb"
      },
      "cell_type": "markdown",
      "source": "# Image Colorization Using Autoencoders and Resnet\n\nThis notebook is an attempt to colorize grayscale images using deep learning. While I originally played around with this idea using an autoencoder and my own steam, I switched to the trying technique discribed in this article: https://medium.freecodecamp.org/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom skimage.color import rgb2gray, gray2rgb, rgb2lab, lab2rgb\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model, load_model,Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, UpSampling2D, RepeatVector, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nimport tensorflow as tf\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "afd747a4d1acc8df76c46f9dad0c1f1df24cea1c"
      },
      "cell_type": "markdown",
      "source": "# Read in Data\n\nThe dataset I am using is ~2000 classic paintings which we will remove the color from and attempt to teach a nearal network to recolorize them."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8f3d29f7d5609caff26b9def320e4a76611f5202"
      },
      "cell_type": "code",
      "source": "IMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 3\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, 1)\nTRAIN_PATH = '../input/art-images-drawings-painting-sculpture-engraving/dataset/dataset_updated/training_set/painting/'\n\ntrain_ids = next(os.walk(TRAIN_PATH))[2]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4528d17eb9a9a078640b150bcc92dc2de7e8e71b"
      },
      "cell_type": "markdown",
      "source": "(Note that 86 of the train_ids have errors while being loading into our dataset, so we will just skip over them. We don't really need them.)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ce8d3f5568e68e131a91fdb97a2f876fe0a8ed4"
      },
      "cell_type": "code",
      "source": "%%time\nX_train = np.zeros((len(train_ids)-86, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nmissing_count = 0\nprint('Getting train images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_+''\n    try:\n        img = imread(path)\n        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n        X_train[n-missing_count] = img\n    except:\n#         print(\" Problem with: \"+path)\n        missing_count += 1\n\nX_train = X_train.astype('float32') / 255.\nprint(\"Total missing: \"+ str(missing_count))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7f44052eda740b3813309b65696a84581443ad1"
      },
      "cell_type": "code",
      "source": "imshow(X_train[5])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0e35f7a6f7f12cdbd4ae2d700b264877ab3f52ff"
      },
      "cell_type": "markdown",
      "source": "# Train/Test Split\nJust getting a sample of 20 images to test the model when it is done."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0415a85b1d4897576877179bb7a6e942d803ced9"
      },
      "cell_type": "code",
      "source": "X_train, X_test = train_test_split(X_train, test_size=20, random_state=seed)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eabcca55a39fca643df6ab142cd26ff5259ec384"
      },
      "cell_type": "markdown",
      "source": "# Create the Model\n\nThe model is a combination of an autoencoder and resnet classifier. The best an autoencoder by itself is just shade everything in a brownish tone. The model uses an resnet classifier to give the neural network an \"idea\" of what things should be colored."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0fda6a03d48ee04f46fc05656d009f75eabedab"
      },
      "cell_type": "code",
      "source": "inception = InceptionResNetV2(weights=None, include_top=True)\ninception.load_weights('../input/inception-resnet-v2-weights/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\ninception.graph = tf.get_default_graph()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18df8319e5666b79f9e2997c4babf6fcdf5d63a6"
      },
      "cell_type": "code",
      "source": "def Colorize():\n    embed_input = Input(shape=(1000,))\n    \n    #Encoder\n    encoder_input = Input(shape=(256, 256, 1,))\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(encoder_input)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    \n    #Fusion\n    fusion_output = RepeatVector(32 * 32)(embed_input) \n    fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n    fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n    fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n    \n    #Decoder\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(64, (4,4), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(32, (2,2), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    return Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n\nmodel = Colorize()\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "be004e23a3ea24ef3c6ab07bf5583e4f662c997e"
      },
      "cell_type": "markdown",
      "source": "# Data Generator Functions"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93752f041d5c365d4b0cf901154daf791dae08f0"
      },
      "cell_type": "code",
      "source": "%%time\n\n# Image transformer\ndatagen = ImageDataGenerator(\n        shear_range=0.2,\n        zoom_range=0.2,\n        rotation_range=20,\n        horizontal_flip=True)\n\n#Create embedding\ndef create_inception_embedding(grayscaled_rgb):\n    def resize_gray(x):\n        return resize(x, (299, 299, 3), mode='constant')\n    grayscaled_rgb_resized = np.array([resize_gray(x) for x in grayscaled_rgb])\n    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n    with inception.graph.as_default():\n        embed = inception.predict(grayscaled_rgb_resized)\n    return embed\n\n#Generate training data\ndef image_a_b_gen(dataset=X_train, batch_size = 20):\n    for batch in datagen.flow(dataset, batch_size=batch_size):\n        X_batch = rgb2gray(batch)\n        grayscaled_rgb = gray2rgb(X_batch)\n        lab_batch = rgb2lab(batch)\n        X_batch = lab_batch[:,:,:,0]\n        X_batch = X_batch.reshape(X_batch.shape+(1,))\n        Y_batch = lab_batch[:,:,:,1:] / 128\n        yield [X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch\n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "396c296aca3e414f269cb47359dfa65f008c979e"
      },
      "cell_type": "markdown",
      "source": "# Checkpoints"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e212da6f4b8651fde538066174d860aefe9c5b2"
      },
      "cell_type": "code",
      "source": "# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\nfilepath = \"Art_Colorization_Model.h5\"\ncheckpoint = ModelCheckpoint(filepath,\n                             save_best_only=True,\n                             monitor='loss',\n                             mode='min')\n\nmodel_callbacks = [learning_rate_reduction,checkpoint]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a799ebe869de0abefc484deb5b5b80ddaf8ca916"
      },
      "cell_type": "markdown",
      "source": "# Train the Model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f2bbd8c98f30c143fba98c77502b482c4281475"
      },
      "cell_type": "code",
      "source": "%%time\nBATCH_SIZE = 20\nmodel.fit_generator(image_a_b_gen(X_train,BATCH_SIZE),\n            epochs=30,\n            verbose=1,\n            steps_per_epoch=X_train.shape[0]/BATCH_SIZE,\n             callbacks=model_callbacks\n                   )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f710bcb0ebd374f0e2117732af5e03a7d2b850a2"
      },
      "cell_type": "code",
      "source": "model.save(filepath)\nmodel.save_weights(\"Art_Colorization_Weights.h5\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "731b1d623a4835b659ab77377cbc5c00c5d7a401"
      },
      "cell_type": "markdown",
      "source": "# Sample the Results"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe0347e93e23d2af2abb3411e391d31461a41de8"
      },
      "cell_type": "code",
      "source": "sample = X_test\ncolor_me = gray2rgb(rgb2gray(sample))\ncolor_me_embed = create_inception_embedding(color_me)\ncolor_me = rgb2lab(color_me)[:,:,:,0]\ncolor_me = color_me.reshape(color_me.shape+(1,))\n\noutput = model.predict([color_me, color_me_embed])\noutput = output * 128\n\ndecoded_imgs = np.zeros((len(output),256, 256, 3))\n\nfor i in range(len(output)):\n    cur = np.zeros((256, 256, 3))\n    cur[:,:,0] = color_me[i][:,:,0]\n    cur[:,:,1:] = output[i]\n    decoded_imgs[i] = lab2rgb(cur)\n    cv2.imwrite(\"img_\"+str(i)+\".jpg\", lab2rgb(cur))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46a6262281eabf011fece01610efc392f090e6ff"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(20, 6))\nfor i in range(10):\n    # grayscale\n    plt.subplot(3, 10, i + 1)\n    plt.imshow(rgb2gray(X_test)[i].reshape(256, 256))\n    plt.gray()\n    plt.axis('off')\n \n    # recolorization\n    plt.subplot(3, 10, i + 1 +10)\n    plt.imshow(decoded_imgs[i].reshape(256, 256,3))\n    plt.axis('off')\n    \n    # original\n    plt.subplot(3, 10, i + 1 + 20)\n    plt.imshow(X_test[i].reshape(256, 256,3))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d50e2fd6792170dbae22fd1838e1779c3c98ad0a"
      },
      "cell_type": "markdown",
      "source": "# Conclusion\n\nThe results are ok but not perfect, mostly brownish with some interesting splashes of color on the testing set. However this is not too bad all things considered because:\n\n-The data set was on artworks, not photographs, which are less consistent and problematic for classification. resnet might be trained on 1.2 million images but I'm not sure how many victorian ball gowns and such are in it.\n\n-The writer of the original article admits that he had to cherry pick the results to get a good sample of 20 from his ~2500 testing set. Most of his results turned out brownish as well.\n\nThere are other techniques to try in the future, but this is a good stopping place for this notebook. If you enjoyed this notebook, please upvote it and check out my other notebooks."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}